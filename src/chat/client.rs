use std::{collections::{HashMap, VecDeque}, sync::Arc};

use reqwest::{Client, Response};

use super::{
    api::{APIRequest, APIResponse, APIResponseHeaders},
    err::ClientError,
    function::{FunctionCall, FunctionDef, Tool, ToolDef},
    prompt::{Message, MessageContext},
};

/// Main client structure for interacting with the OpenAI API.
#[derive(Clone)]
pub struct OpenAIClient {
    /// HTTP client
    pub client: Client,
    /// API endpoint
    pub end_point: String,
    /// Optional API key
    pub api_key: Option<String>,
    /// Registered tools: key is the tool name, value is a tuple (tool, is_enabled)
    pub tools: HashMap<String, (Arc<dyn Tool + Send + Sync>, bool)>,
    /// Configuration for the model request.
    pub model_config: Option<ModelConfig>,
}

/// Configuration for the model request.
#[derive(Debug, Clone)]
pub struct ModelConfig {
    /// Model name.
    pub model: String,
    /// Optional model name.
    pub model_name: Option<String>,
    /// Top-p sampling parameter.
    pub top_p: Option<f64>,
    /// Specifies whether to perform parallel ToolCalls.
    /// default: true
    pub parallel_tool_calls: Option<bool>,
    /// Specifies the diversity of tokens generated by the model.
    pub temperature: Option<f64>,
    /// Specifies the maximum number of tokens generated by the model.
    pub max_completion_tokens: Option<u64>,
    /// Specifies the level of effort for reasoning in the inference model:
    /// - "low": Low effort
    /// - "medium": Medium effort
    /// - "high": High effort
    /// default: "medium"
    pub reasoning_effort: Option<String>,
    /// Specifies whether to apply a presence penalty to the model.
    /// Range: 2.0..-2.0
    pub presence_penalty: Option<f64>,
    /// Strictly structured
    /// default: false
    /// Forced disabled in parallel ToolCalls
    pub strict: Option<bool>,
}

/// Contains the API response and its headers.
#[derive(Debug, Clone)]
pub struct APIResult {
    /// The parsed API response.
    pub response: APIResponse,
    /// Headers returned by the API.
    pub headers: APIResponseHeaders,
}

impl OpenAIClient {
    /// Create a new OpenAIClient.
    ///
    /// # Arguments
    ///
    /// * `end_point` - The endpoint of the OpenAI API.
    /// * `api_key` - Optional API key.
    pub fn new(end_point: &str, api_key: Option<&str>) -> Self {
        Self {
            client: Client::new(),
            end_point: end_point.trim_end_matches('/').to_string(),
            api_key: api_key.map(|s| s.to_string()),
            tools: HashMap::new(),
            model_config: None,
        }
    }

    /// Set the default model configuration.
    /// 
    /// # Arguments
    /// 
    /// * `model_config` - The model configuration.
    pub fn set_model_config(&mut self, model_config: &ModelConfig) {
        self.model_config = Some(model_config.clone());
    }

    /// Register a tool.
    ///
    /// If a tool with the same name already exists, it will be overwritten.
    ///
    /// # Arguments
    ///
    /// * `tool` - Reference-counted tool implementing the Tool trait.
    pub fn def_tool<T: Tool + Send + Sync + 'static>(&mut self, tool: Arc<T>) {
        self.tools
            .insert(tool.def_name().to_string(), (tool, true));
    }

    /// List all registered tools.
    ///
    /// # Returns
    ///
    /// A list of tuples containing (tool name, tool description, enabled flag).
    pub fn list_tools(&self) -> Vec<(String, String, bool)> {
        let mut tools = Vec::new();
        for (tool_name, (tool, enable)) in self.tools.iter() {
            tools.push((
                tool_name.to_string(),
                tool.def_description().to_string(),
                *enable,
            ));
        }
        tools
    }

    /// Switch the enable/disable state of a tool.
    ///
    /// # Arguments
    ///
    /// * `tool_name` - The name of the tool.
    /// * `t_enable` - True to enable, false to disable.
    pub fn switch_tool(&mut self, tool_name: &str, t_enable: bool) {
        if let Some((_, enable)) = self.tools.get_mut(tool_name) {
            *enable = t_enable;
        }
    }

    /// Export the definitions of all enabled tools.
    ///
    /// # Returns
    ///
    /// A vector of function definitions.
    pub fn export_tool_def(&self) -> Result<Vec<ToolDef>, ClientError> {
        let mut defs = Vec::new();
        for (tool_name, (tool, enable)) in self.tools.iter() {
            if *enable {
                defs.push(ToolDef {
                    tool_type: "function".to_string(),
                    function: FunctionDef {
                        name: tool_name.clone(),
                        description: tool.def_description().to_string(),
                        parameters: tool.def_parameters(),
                        strict: self.model_config.as_ref().ok_or(ClientError::ModelConfigNotSet)?.strict.unwrap_or(false),
                    },
                });
            }
        }
        Ok(defs)
    }

    /// Send a chat request to the API.
    ///
    /// # Arguments
    ///
    /// * `model` - The model configuration.
    /// * `prompt` - A vector of user and system messages.
    ///
    /// # Returns
    ///
    /// The API result or a ClientError.
    pub async fn send(
        &self,
        prompt: &VecDeque<Message>,
        model: Option<&ModelConfig>,
    ) -> Result<APIResult, ClientError> {
        match self
            .call_api(
                prompt,
                Some(&serde_json::json!("none")),
                model,
            )
            .await
        {
            Ok(res) => Ok(res),
            Err(e) => Err(e),
        }
    }

    /// Send a chat request with tool auto-selection.
    ///
    /// # Arguments
    ///
    /// * `model` - The model configuration.
    /// * `prompt` - A vector of messages.
    ///
    /// # Returns
    ///
    /// The API result or a ClientError.
    pub async fn send_can_use_tool(
        &self,
        prompt: &VecDeque<Message>,
        model: Option<&ModelConfig>,
    ) -> Result<APIResult, ClientError> {
        match self
            .call_api(
                prompt,
                Some(&serde_json::json!("auto")),
                model,
            )
            .await
        {
            Ok(res) => Ok(res),
            Err(e) => Err(e),
        }
    }

    /// Send a chat request requiring the use of a tool.
    /// 
    /// # Arguments
    /// 
    /// * `model` - The model configuration.
    /// * `prompt` - A vector of messages.
    /// 
    /// # Returns
    /// 
    /// The API result or a ClientError.
    pub async fn send_use_tool(
        &self,
        prompt: &VecDeque<Message>,
        model: Option<&ModelConfig>,
    ) -> Result<APIResult, ClientError> {
        match self
            .call_api(
                prompt,
                Some(&serde_json::json!("required")),
                model,
            )
            .await
        {
            Ok(res) => Ok(res),
            Err(e) => Err(e),
        }
    }

    /// Send a chat request forcing the use of a specific tool.
    ///
    /// # Arguments
    ///
    /// * `model` - The model configuration.
    /// * `prompt` - A vector of messages.
    /// * `tool_name` - The name of the tool to force.
    ///
    /// # Returns
    ///
    /// The API result or a ClientError.
    pub async fn send_with_tool(
        &self,
        prompt: &VecDeque<Message>,
        tool_name: &str,
        model: Option<&ModelConfig>,
    ) -> Result<APIResult, ClientError> {
        let function_call = serde_json::json!({"type": "function", "function": {"name": tool_name}});

        match self
            .call_api(
                prompt,
                Some(&function_call),
                model,
            )
            .await
        {
            Ok(res) => Ok(res),
            Err(e) => Err(e),
        }
    }

    /// Calls the OpenAI chat completions API.
    ///
    /// # Arguments
    ///
    /// * `model` - The model name; e.g. "GPT-4o".
    /// * `prompt` - The list of messages.
    /// * `function_call` - Indicates function call mode:
    ///   - "auto"
    ///   - "none"
    ///   - { "name": "get_weather" }
    /// * `temp` - Temperature parameter.
    /// * `max_token` - Maximum tokens parameter.
    /// * `top_p` - Top-p sampling parameter.
    ///
    /// # Returns
    ///
    /// An APIResult on success or a ClientError on failure.
    pub async fn call_api(
        &self,
        prompt: &VecDeque<Message>,
        tool_choice: Option<&serde_json::Value>,
        model_config: Option<&ModelConfig>,
    ) -> Result<APIResult, ClientError> {
        let url = format!("{}/chat/completions", self.end_point);
        if !url.starts_with("https://") && !url.starts_with("http://") {
            return Err(ClientError::InvalidEndpoint);
        }

        let model_config = model_config.unwrap_or(self.model_config.as_ref().ok_or(ClientError::ModelConfigNotSet)?);
        let tools = self.export_tool_def()?;
        let res = self.request_api(&self.end_point, self.api_key.as_deref(), model_config, prompt, &tools, tool_choice.unwrap_or(&serde_json::Value::Null)).await?;

        let headers = APIResponseHeaders {
            retry_after: res
                .headers()
                .get("Retry-After")
                .and_then(|v| v.to_str().ok().and_then(|v| v.parse().ok())),
            reset: res
                .headers()
                .get("X-RateLimit-Reset")
                .and_then(|v| v.to_str().ok().and_then(|v| v.parse().ok())),
            rate_limit: res
                .headers()
                .get("X-RateLimit-Remaining")
                .and_then(|v| v.to_str().ok().and_then(|v| v.parse().ok())),
            limit: res
                .headers()
                .get("X-RateLimit-Limit")
                .and_then(|v| v.to_str().ok().and_then(|v| v.parse().ok())),
            extra_other: res
                .headers()
                .iter()
                .map(|(k, v)| {
                    (
                        k.as_str().to_string(),
                        v.to_str().unwrap_or("").to_string(),
                    )
                })
                .collect(),
        };
        let text = res.text().await.map_err(|_| ClientError::InvalidResponse)?;
        log::debug!("Response: {}", text);
        let response_body: APIResponse =
            serde_json::from_str(&text).map_err(|_| {
            ClientError::InvalidResponse
            })?;

        Ok(APIResult {
            response: response_body,
            headers,
        })
    }

    pub async fn request_api(&self ,end_point: &str, api_key: Option<&str>, model_config: &ModelConfig ,message: &VecDeque<Message>, tools: &Vec<ToolDef>, tool_choice: &serde_json::Value) -> Result<Response, ClientError> {
        let request = APIRequest {
            model:                  model_config.model.clone(),
            messages:               message.clone(),
            tools:                  tools.clone(),
            tool_choice:            tool_choice.clone(),
            parallel_tool_calls:    model_config.parallel_tool_calls,
            temperature:            model_config.temperature,
            max_completion_tokens:  model_config.max_completion_tokens,
            top_p:                  model_config.top_p,
            reasoning_effort:       model_config.reasoning_effort.clone(),
            presence_penalty:       model_config.presence_penalty,
        };

        let res = self
            .client
            .post(&format!("{}/chat/completions", end_point))
            .header("Content-Type", "application/json")
            .header(
                "authorization",
                format!("Bearer {}", api_key.as_deref().unwrap_or("")),
            )
            .json(&request)
            .send()
            .await
            .map_err(|_| ClientError::NetworkError)?;

        Ok(res)
    }

    /// Create a new prompt conversation.
    ///
    /// # Returns
    ///
    /// A new OpenAIClientState with an empty message history.
    pub fn create_prompt(&self) -> OpenAIClientState {
        OpenAIClientState {
            prompt: VecDeque::new(),
            client: self.clone(),
            entry_limit: None,
        }
    }
}

/// Represents a client state with a prompt history.
#[derive(Clone)]
pub struct OpenAIClientState {
    /// Conversation history messages.
    pub prompt: VecDeque<Message>,
    /// Reference to the OpenAIClient.
    pub client: OpenAIClient,
    pub entry_limit: Option<u64>,
}

#[derive(Debug, Clone)]
pub struct GenerateResponse {
    pub has_content: bool,
    pub has_tool_calls: bool,
    pub content: Option<String>,
    pub tool_calls: Option<Vec<FunctionCall>>,
    pub api_result: APIResult,
}

impl<'a> OpenAIClientState {
    /// Add messages to the conversation prompt.
    ///
    /// # Arguments
    ///
    /// * `messages` - A vector of messages to add.
    ///
    /// # Returns
    ///
    /// A mutable reference to self.
    pub async fn add(&mut self, messages: Vec<Message>) -> &mut Self {
        if let Some(limit) = self.entry_limit {
            while self.prompt.len() as u64 + messages.len() as u64 > limit {
                self.prompt.pop_front();
            }
        }
        self.prompt.extend(messages);
        self
    }

    /// Set the maximum number of entries in the conversation prompt.
    ///    
    /// # Arguments
    /// 
    /// * `limit` - The maximum number of entries.
    /// 
    /// # Returns
    /// 
    /// A mutable reference to self.
    pub async fn set_entry_limit(&mut self, limit: u64) -> &mut Self {
        self.entry_limit = Some(limit);
        while self.prompt.len() as u64 > limit {
            self.prompt.pop_front();
        }
        self
    }

    /// Clear all messages from the conversation prompt.
    ///
    /// # Returns
    ///
    /// A mutable reference to self.
    pub async fn clear(&mut self) -> &mut Self {
        self.prompt.clear();
        self
    }

    /// Retrieve the last message in the prompt.
    ///
    /// # Returns
    ///
    /// An Option containing a reference to the last Message.
    pub async fn last(&mut self) -> Option<&Message> {
        self.prompt.back()
    }

    /// Generate an AI response.
    ///
    /// This method sends the prompt to the API and, upon successful response,
    /// adds the assistant's message to the prompt.
    ///
    /// # Arguments
    ///
    /// * `model` - The model configuration.
    ///
    /// # Returns
    ///
    /// An APIResult with the API response or a ClientError.
    pub async fn generate(&mut self, model: Option<&ModelConfig>) -> Result<GenerateResponse, ClientError> {
        // Retrieve model configuration: use provided model or fallback to the client's config.
        let model = model.unwrap_or(
            self.client
                .model_config
                .as_ref()
                .ok_or(ClientError::ModelConfigNotSet)?
        );

        // Send the request and extract the first choice.
        let result = self.client.send(&self.prompt, Some(model)).await?;
        let choice = result
            .response
            .choices
            .as_ref()
            .and_then(|choices| choices.first())
            .ok_or(ClientError::InvalidResponse)?;

        // Ensure there is content in the assistant's reply.
        let content = choice
            .message
            .content
            .as_ref()
            .ok_or(ClientError::UnknownError)?;

        // Add the assistant's message to the conversation.
        self.add(vec![Message::Assistant {
            name: model.model_name.clone(),
            content: vec![MessageContext::Text(content.clone())],
            tool_calls: None,
        }])
        .await;

        Ok(
            GenerateResponse {
                has_content: true,
                has_tool_calls: false,
                content: Some(content.clone()),
                tool_calls: None,
                api_result: result,
            }
        )
    }

    /// Generate an AI response, possibly calling a tool.
    ///
    /// If the API response includes a function call, it will run the corresponding tool.
    ///
    /// # Arguments
    ///
    /// * `model` - The model configuration.
    /// * `show_call` - Optional callback function to show the tool call.(eg, `show_call("tool_name", "args")`)
    ///
    /// # Returns
    ///
    /// An APIResult with the API response or a ClientError.
    pub async fn generate_can_use_tool<F>(&mut self, model: Option<&ModelConfig>, show_call: Option<F>) -> Result<GenerateResponse, ClientError>
    where F: Fn(&str, &serde_json::Value) { 
        // Use the provided model configuration or fallback to the client's configuration.
        let model = model.or(self.client.model_config.as_ref()).ok_or(ClientError::ModelConfigNotSet)?;

        // Send the request with "can use tool" mode.
        let result = self.client.send_can_use_tool(&self.prompt, Some(model)).await?;
        let choices = result
            .response
            .choices
            .as_ref()
            .ok_or(ClientError::InvalidResponse)?;
        
        let choice = choices.first().ok_or(ClientError::InvalidResponse)?;
        let has_content = choice.message.content.is_some();
        let has_tool_calls = choice.message.tool_calls.is_some();

        // Ensure that there is either content or a tool call.
        if !has_content && !has_tool_calls {
            return Err(ClientError::UnknownError);
        }

        // If content is returned, add the assistant message.
        self.add(vec![Message::Assistant {
            name: model.model_name.clone(),
            content: if has_content { vec![MessageContext::Text(choice.message.content.clone().unwrap())] } else { vec![] },
            tool_calls: choice.message.tool_calls.clone(),
        }]).await;

        // Process any tool calls.
        if let Some(tool_calls) = &choice.message.tool_calls {
            for call in tool_calls {
                let (tool, enabled) = self.client.tools
                    .get(&call.function.name)
                    .ok_or(ClientError::ToolNotFound)?;
                if !*enabled {
                    return Err(ClientError::ToolNotFound);
                }
                if let Some(show_call) = &show_call {
                    show_call(&call.function.name, &call.function.arguments);
                }
                let result_text = tool
                    .run(call.function.arguments.clone())
                    .unwrap_or_else(|e| format!("Error: {}", e));
                self.add(vec![Message::Tool {
                    tool_call_id: call.id.clone(),
                    content: vec![MessageContext::Text(result_text)],
                }]).await;
            }
        }

        Ok(GenerateResponse {
            has_content,
            has_tool_calls,
            content: choice.message.content.clone(),
            tool_calls: choice.message.tool_calls.clone(),
            api_result: result,
        })
    }

    /// Generate an AI response while forcing the use of a specific tool.
    /// 
    /// If the response includes a function call, the specified tool will be executed
    /// 
    /// # Arguments
    /// 
    /// * `model` - The model configuration.
    /// * `tool_name` - The name of the tool to use.
    /// * `show_call` - Optional callback function to show the tool call.(eg, `show_call("tool_name", "args")`)
    /// 
    /// # Returns
    /// 
    /// An APIResult with the API response or a ClientError.
    pub async fn generate_use_tool<F>(&mut self, model: Option<&ModelConfig>, show_call: Option<F>) -> Result<GenerateResponse, ClientError>
    where F: Fn(&str, &serde_json::Value) {
        let model = model.unwrap_or(
            self.client
                .model_config
                .as_ref()
                .ok_or(ClientError::ModelConfigNotSet)?
        );

        let result = self.client.send_use_tool(&self.prompt, Some(model)).await?;
        let choices = result
            .response
            .choices
            .as_ref()
            .ok_or(ClientError::InvalidResponse)?;

        let choice = choices.first().ok_or(ClientError::InvalidResponse)?;
        let content = choice.message.content.clone();
        let tool_calls = choice.message.tool_calls.clone();

        // If there is no tool call, return an error.
        if tool_calls.is_none() {
            return Err(ClientError::ToolNotFound);
        }

        let has_content = content.is_some();

        // Add the assistant's reply to the conversation.
        self.add(vec![Message::Assistant {
            name: model.model_name.clone(),
            content: if has_content { vec![MessageContext::Text(choice.message.content.clone().unwrap())] } else { vec![] },
            tool_calls: choice.message.tool_calls.clone(),
        }]).await;

        // Process any tool calls.
        if let Some(calls) = tool_calls.clone() {
            for call in calls {
                let (tool, enabled) = self
                    .client
                    .tools
                    .get(&call.function.name)
                    .ok_or(ClientError::ToolNotFound)?;
                if !*enabled {
                    return Err(ClientError::ToolNotFound);
                }
                if let Some(show_call) = &show_call {
                    show_call(&call.function.name, &call.function.arguments);
                }
                let result_text = match tool.run(call.function.arguments.clone()) {
                    Ok(res) => res,
                    Err(e) => format!("Error: {}", e),
                };
                self.add(vec![Message::Tool {
                    tool_call_id: call.id.clone(),
                    content: vec![MessageContext::Text(result_text)],
                }]).await;
            }
        }

        Ok(GenerateResponse {
            has_content,
            has_tool_calls: true,
            content,
            tool_calls,
            api_result: result,
        })
    }

    /// Generate an AI response while forcing the use of a specific tool.
    ///
    /// If the response includes a function call, the specified tool will be executed.
    ///
    /// # Arguments
    ///
    /// * `model` - The model configuration.
    /// * `tool_name` - The name of the tool to use.
    /// * `show_call` - Optional callback function to show the tool call.(eg, `show_call("tool_name", "args")`)
    ///
    /// # Returns
    ///
    /// An APIResult with the API response or a ClientError.
    pub async fn generate_with_tool<F>(&mut self, model: Option<&ModelConfig>, tool_name: &str, show_call: Option<F>) -> Result<GenerateResponse, ClientError>
    where F: Fn(&str, &serde_json::Value) {
        let model = model.unwrap_or(
            self.client.model_config.as_ref().ok_or(ClientError::ModelConfigNotSet)?
        );

        let result = self.client.send_with_tool(&self.prompt, tool_name, Some(model)).await?;
        let choices = result
            .response
            .choices
            .as_ref()
            .ok_or(ClientError::InvalidResponse)?;

        let choice = choices.first().ok_or(ClientError::InvalidResponse)?;
        let content = choice.message.content.clone();
        let tool_calls = choice.message.tool_calls.clone();

        // If there is no tool call, return an error.
        if tool_calls.is_none() {
            return Err(ClientError::ToolNotFound);
        }

        let has_content = content.is_some();

        // Add the assistant's reply to the conversation.
        self.add(vec![Message::Assistant {
            name: model.model_name.clone(),
            content: if has_content { vec![MessageContext::Text(choice.message.content.clone().unwrap())] } else { vec![] },
            tool_calls: choice.message.tool_calls.clone(),
        }]).await;

        // Process any tool calls.
        if let Some(calls) = tool_calls.clone() {
            for call in calls {
                let (tool, enabled) = self
                    .client
                    .tools
                    .get(&call.function.name)
                    .ok_or(ClientError::ToolNotFound)?;
                if !*enabled {
                    return Err(ClientError::ToolNotFound);
                }
                if let Some(show_call) = &show_call {
                    show_call(&call.function.name, &call.function.arguments);
                }
                let result_text = match tool.run(call.function.arguments.clone()) {
                    Ok(res) => res,
                    Err(e) => format!("Error: {}", e),
                };
                self.add(vec![Message::Tool {
                    tool_call_id: call.id.clone(),
                    content: vec![MessageContext::Text(result_text)],
                }]).await;
            }
        }

        Ok(
            GenerateResponse {
                has_content,
                has_tool_calls: true,
                content,
                tool_calls,
                api_result: result,
            }
        )
    }
}